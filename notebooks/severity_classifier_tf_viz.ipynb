{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60189e1",
   "metadata": {},
   "source": [
    "# Customer Complaint **Severity Classification** (TensorFlow)\n",
    "\n",
    "**Objective:** Train a deep learning text classifier to predict complaint **severity** (`low`, `medium`, `high`) from narratives using **weak supervision** via TextBlob sentiment.\n",
    "\n",
    "**Pipeline:** Load → Clean → Pseudo-label (TextBlob) → Tokenize/Pad → Train LSTM → Evaluate → Save.\n",
    "\n",
    "**Inputs required:** path to the CSV with at least one text column named `narrative`.\n",
    "\n",
    "> Note: This notebook does not build a dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed. Uncomment when running locally.\n",
    "# %pip install -q tensorflow==2.15.0.post1 pandas scikit-learn textblob matplotlib\n",
    "\n",
    "import os, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e447ae",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f31144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== User-configurable paths and hyperparameters ====\n",
    "CSV_PATH = os.getenv('CSV_PATH', 'complaints_processed.csv')  # change if needed\n",
    "TEXT_COLUMN = 'narrative'\n",
    "MIN_CHARS = 20            # drop very short narratives\n",
    "MAX_SAMPLES = None        # set int to subsample for quicker runs\n",
    "\n",
    "# Tokenization\n",
    "MAX_WORDS = 30000         # vocab size\n",
    "MAX_LEN = 160             # sequence length\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 128\n",
    "LSTM_UNITS = 96\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 8                # increase if you have GPU/time\n",
    "\n",
    "# Training\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output\n",
    "MODEL_DIR = 'artifacts'\n",
    "MODEL_BASENAME = 'severity_lstm_tf'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "print('CSV_PATH =', CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbaf448",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceacdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "expected_cols = {TEXT_COLUMN}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f'Missing required columns: {missing}. Found: {list(df.columns)}')\n",
    "\n",
    "# Basic cleaning: drop NA and very short entries\n",
    "df = df.dropna(subset=[TEXT_COLUMN]).copy()\n",
    "df[TEXT_COLUMN] = df[TEXT_COLUMN].astype(str).str.strip()\n",
    "df = df[df[TEXT_COLUMN].str.len() >= MIN_CHARS]\n",
    "\n",
    "# Optional subsample for speed\n",
    "if MAX_SAMPLES is not None and len(df) > MAX_SAMPLES:\n",
    "    df = df.sample(MAX_SAMPLES, random_state=RANDOM_STATE)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be75dc2",
   "metadata": {},
   "source": [
    "## Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19149d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "_url_pat = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "_email_pat = re.compile(r'\\S+@\\S+')\n",
    "_nonprint_pat = re.compile(r'[^\\x00-\\x7F]+')  # drop non-ascii to avoid tokenizer oddities\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = _url_pat.sub(' ', s)\n",
    "    s = _email_pat.sub(' ', s)\n",
    "    s = _nonprint_pat.sub(' ', s)\n",
    "    s = re.sub(r'[^a-z0-9\\s\\.\\,\\!\\?\\$]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df['text'] = df[TEXT_COLUMN].apply(clean_text)\n",
    "df[['text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18013c59",
   "metadata": {},
   "source": [
    "## Weak supervision: map TextBlob polarity to severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_to_severity(p: float, text: str) -> str:\n",
    "    # Thresholds chosen by quick inspection; tweak as needed.\n",
    "    # Add hard triggers for escalation keywords.\n",
    "    t = text.lower()\n",
    "    escalators = [\n",
    "        'fraud','scam','lawsuit','legal','attorney','regulator',\n",
    "        'chargeback','bbb','complaint filed','not resolved','escalate',\n",
    "        'cancel my account','close my account','lost my money','stolen'\n",
    "    ]\n",
    "    if any(k in t for k in escalators) or p < -0.35:\n",
    "        return 'high'\n",
    "    if -0.35 <= p < 0.05:\n",
    "        return 'medium'\n",
    "    return 'low'\n",
    "\n",
    "def map_severity_series(texts: pd.Series) -> pd.Series:\n",
    "    pols = texts.apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    return pd.Series([\n",
    "        polarity_to_severity(p, t) for p, t in zip(pols, texts)\n",
    "    ], index=texts.index)\n",
    "\n",
    "df['severity'] = map_severity_series(df['text'])\n",
    "df['severity'].value_counts(normalize=True).mul(100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b64ec",
   "metadata": {},
   "source": [
    "## Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'low':0,'medium':1,'high':2}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "df['label'] = df['severity'].map(label2id)\n",
    "assert df['label'].isna().sum() == 0\n",
    "df[['text','severity','label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0612ebb",
   "metadata": {},
   "source": [
    "## Train/validation/test split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df['text'], df['label'], test_size=(VAL_SIZE + TEST_SIZE),\n",
    "    random_state=RANDOM_STATE, stratify=df['label']\n",
    ")\n",
    "rel_test = TEST_SIZE / (VAL_SIZE + TEST_SIZE)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=rel_test, random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "print(len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c13d3",
   "metadata": {},
   "source": [
    "## Tokenize and pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train.tolist())\n",
    "\n",
    "def to_seq(texts):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "Xtr = to_seq(X_train)\n",
    "Xv = to_seq(X_val)\n",
    "Xte = to_seq(X_test)\n",
    "ytr = np.array(y_train)\n",
    "yv = np.array(y_val)\n",
    "yte = np.array(y_test)\n",
    "\n",
    "vocab_size = min(MAX_WORDS, len(tokenizer.word_index) + 1)\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba7f0a",
   "metadata": {},
   "source": [
    "## Handle class imbalance with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "classes = np.array(sorted(label2id.values()))\n",
    "class_weights_arr = compute_class_weight(class_weight='balanced', classes=classes, y=ytr)\n",
    "class_weights = {int(c): float(w) for c, w in zip(classes, class_weights_arr)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c33df",
   "metadata": {},
   "source": [
    "## Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b50cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=EMBED_DIM, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(LSTM_UNITS, return_sequences=False)),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb8db9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f01a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(MODEL_DIR, f'{MODEL_BASENAME}.keras')\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1),\n",
    "    ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    Xtr, ytr,\n",
    "    validation_data=(Xv, yv),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.xlabel('epoch'); plt.ylabel('accuracy'); plt.legend(); plt.title('Accuracy'); plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(); plt.title('Loss'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ad6c4",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict(Xte, batch_size=512)\n",
    "pred = probs.argmax(axis=1)\n",
    "print(classification_report(yte, pred, target_names=[id2label[i] for i in range(3)]))\n",
    "cm = confusion_matrix(yte, pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ad085",
   "metadata": {},
   "source": [
    "## Save model, tokenizer, and label maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad02a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Keras model\n",
    "model_out = os.path.join(MODEL_DIR, f'{MODEL_BASENAME}.keras')\n",
    "model.save(model_out)\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "tok_out = os.path.join(MODEL_DIR, f'{MODEL_BASENAME}_tokenizer.pkl')\n",
    "with open(tok_out, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save label maps\n",
    "with open(os.path.join(MODEL_DIR, f'{MODEL_BASENAME}_labels.json'), 'w') as f:\n",
    "    json.dump({'label2id': label2id, 'id2label': id2label}, f)\n",
    "\n",
    "print('Saved:', model_out)\n",
    "print('Saved:', tok_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cd8fb",
   "metadata": {},
   "source": [
    "## Inference helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_severity(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    cleaned = [clean_text(t) for t in texts]\n",
    "    seq = to_seq(cleaned)\n",
    "    probs = model.predict(seq)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    labels = [id2label[i] for i in preds]\n",
    "    return list(zip(labels, probs.max(axis=1).round(4)))\n",
    "\n",
    "# Example\n",
    "predict_severity([\n",
    "    'They stole my money and refuse to refund. I will file with the regulator.',\n",
    "    'App login failed once but works now.',\n",
    "    'Customer support is slow and I am disappointed.'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad06ec",
   "metadata": {},
   "source": [
    "## Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Class distribution visualization\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.countplot(x='severity', data=df, order=['low','medium','high'], palette='Blues')\n",
    "plt.title('Severity Class Distribution')\n",
    "plt.xlabel('Severity Level')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152229e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Confusion Matrix Heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(yte, pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[id2label[i] for i in range(3)],\n",
    "            yticklabels=[id2label[i] for i in range(3)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Word Clouds per severity level\n",
    "from wordcloud import WordCloud\n",
    "for label in ['low','medium','high']:\n",
    "    text_blob = ' '.join(df[df['severity']==label]['text'].tolist())\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate(text_blob)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'WordCloud - {label.upper()} severity')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
